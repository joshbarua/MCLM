{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from src.prompts import language_dict\n",
    "\n",
    "def get_keys_by_value(d, target_value):\n",
    "    return [key for key, value in d.items() if value == target_value][0]\n",
    "\n",
    "languages = [\"Afrikaans\", \"Arabic\", \"Chinese_(Simplified)\", \"English\", \"French\", \"German\", \"Hebrew\", \"Indonesian\", \"Italian\", \"Japanese\", \"Korean\", \"Spanish\", \"Turkish\", \"Vietnamese\"]\n",
    "models = [\n",
    "    \"jwhj/Qwen2.5-Math-1.5B-OREO\", \"nvidia/AceMath-1.5B-Instruct\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"Qwen/Qwen2.5-Math-1.5B-Instruct\",\n",
    "    \"PRIME-RL/Eurus-2-7B-PRIME\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"nvidia/AceMath-7B-Instruct\", \"Qwen/Qwen2.5-Math-7B-Instruct\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\", \"o3-mini\"\n",
    "]\n",
    "iso = [get_keys_by_value(language_dict, l) for l in languages]\n",
    "\n",
    "res = {\"model\": models}\n",
    "for i, l in enumerate(languages):\n",
    "    # for task in [\"math500\", \"math100\"]:\n",
    "    #     df = pd.read_csv(os.path.join(\"score_result\", f\"{task}.csv\"))\n",
    "    #     res[f\"{l}_{task[-3:]}\"] = list(df[l])\n",
    "    df_500, df_100 = pd.read_csv(os.path.join(\"score_result\", \"math500.csv\")), pd.read_csv(os.path.join(\"score_result\", \"math100.csv\"))\n",
    "    res[iso[i]] = []\n",
    "    for m in models:\n",
    "        m = m.replace(\"/\", \"_\")\n",
    "        df_500_index, df_100_index = list(df_500[\"model\"]).index(m), list(df_100[\"model\"]).index(m)\n",
    "        # res[iso[i]].append(abs(list(df_500[l])[df_500_index] - list(df_100[l])[df_100_index]))\n",
    "        res[iso[i]].append(list(df_500[l])[df_500_index])\n",
    "res = pd.DataFrame(res)\n",
    "res.to_excel(\"check.xlsx\", index=False)\n",
    "# .to_csv(f\"result_{task}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Evaluation Score T-test ---------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.9555555555555554, pvalue=5.5114638447971785e-06)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "import pickle\n",
    "\n",
    "with open(\"boot.pickle\", \"rb\") as f:\n",
    "    a = pickle.load(f)\n",
    "\n",
    "print(f\"--------------------- Evaluation Score T-test ---------------------\")\n",
    "check_500, check_100 = [], []\n",
    "for key in a[\"math500\"].keys():\n",
    "    # tau, p_value = scipy.stats.kendalltau(a[\"math500\"][key], a[\"math100\"][key])  # Welch's t-test 적용\n",
    "    check_500.append(np.mean(a[\"math500\"][key]))\n",
    "    check_100.append(np.mean(a[\"math100\"][key]))\n",
    "    # print(f\"{key} - Kendall-Tau: {tau:.4f}, P-Value: {p_value:.4f}\")\n",
    "scipy.stats.kendalltau(check_500, check_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:04<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Fleiss' kappa (multi-rater agreement): 0.598\n",
      "Qwen2.5-Math-1.5B-Instruct - math100:  0.5982547552960662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Fleiss' kappa (multi-rater agreement): 0.618\n",
      "Qwen2.5-Math-1.5B-Instruct - aime2024:  0.6178097740597748\n"
     ]
    }
   ],
   "source": [
    "from metrics import evaluate_consistency\n",
    "import os\n",
    "\n",
    "root_path = \"prm_result/1.5B/prm_72B_5_8\"\n",
    "for task in [\"math100\", \"aime2024\"]:\n",
    "    a = evaluate_consistency(os.path.join(root_path, task, \"text-completion-openai_Qwen_Qwen2.5-Math-1.5B-Instruct\"))\n",
    "    print(f\"Qwen2.5-Math-1.5B-Instruct - {task}: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import batch_completion\n",
    "from src.prompts import judge_template, language_dict\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_xxx\"\n",
    "\n",
    "def get_keys_by_value(d, target_value):\n",
    "    return [key for key, value in d.items() if value == target_value][0]\n",
    "\n",
    "path_list = [\n",
    "    \"prm_result/1.5B/prm_72B_5_8\", # 이 path 내부에 task로 나뉘는 형식\n",
    "    # \"results\"\n",
    "]\n",
    "save_path, log_path = \"new_mo_result\", \"new_mo_logs\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "for root_path in path_list:\n",
    "    if root_path == \"prm_result/1.5B/prm_72B_5_8\":\n",
    "        model = \"text-completion-openai_Qwen_Qwen2.5-Math-1.5B-Instruct\"\n",
    "    elif root_path == \"results\":\n",
    "        model = \"deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "    for data in [\"IMO\", \"MMO\"]:\n",
    "        if data == \"IMO\":\n",
    "            dataset = load_dataset(\"OLAIR/M-IMO-extended\", split=\"train\").to_pandas()\n",
    "        res = {\n",
    "            \"question\": [],\n",
    "            \"response\": [],\n",
    "            \"answer\": [],\n",
    "            \"model\": [],\n",
    "            \"language\": [],\n",
    "            \"input_prompt\": [],\n",
    "            \"judge\": []\n",
    "        }\n",
    "        languages = list(language_dict.values())\n",
    "        result_dict = {key: [] for key in languages}\n",
    "        for lang in languages:\n",
    "            if (lang == \"Chinese_(Simplified)\") and (data == \"MMO\"):\n",
    "                lang = \"Chinese\"\n",
    "            if os.path.exists(os.path.join(root_path, data, model, f\"{lang}.jsonl\")):\n",
    "                df = pd.read_json(os.path.join(root_path, data, model, f\"{lang}.jsonl\"), lines=True)\n",
    "                for _,row in df.iterrows():\n",
    "                    res[\"model\"].append(model)\n",
    "                    res[\"language\"].append(lang)\n",
    "                    res[\"question\"].append(row[\"question\"])\n",
    "                    res[\"response\"].append(row[\"response\"])\n",
    "                    res[\"answer\"].append(row[\"answer\"])\n",
    "                    res[\"input_prompt\"].append([\n",
    "                        {\"role\": \"system\", \"content\": \"You are a good judge.\"},\n",
    "                        {\"role\": \"user\", \"content\": judge_template.replace(\"<math_question>\", str(row[\"question\"])).replace(\"<correct_answer>\", str(row[\"answer\"])).replace(\"<model_solution>\", str(row[\"response\"]))}\n",
    "                    ])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        print(f\"{data} is Evaluating!\")\n",
    "        outputs = batch_completion(\n",
    "            messages=res[\"input_prompt\"],\n",
    "            temperature=0,\n",
    "            max_tokens=4096,\n",
    "            model=\"gpt-4o-mini\"\n",
    "        )\n",
    "        for output in outputs:\n",
    "            try:\n",
    "                res[\"judge\"].append(output.choices[0].message.content)\n",
    "            except:\n",
    "                res[\"judge\"].append(\"\")\n",
    "        res = pd.DataFrame(res)\n",
    "        res.to_csv(f\"{log_path}/{data}-{model}_log.csv\", index=False)\n",
    "        print(f\"{data} Evaluation is done!\")\n",
    "\n",
    "        for lang in languages:\n",
    "            if (lang in list(res[\"language\"])) or (\"Chinese\" in lang):\n",
    "                if (\"Chinese\" in lang) and (\"MMO\" in data):\n",
    "                    original_lang = lang\n",
    "                    lang = \"Chinese\"\n",
    "                subset = res[res[\"language\"] == lang]\n",
    "                subset.reset_index(inplace=True, drop=True)\n",
    "                true, false = 0, 0\n",
    "                for i,row in subset.iterrows():\n",
    "                    if \"IMO\" in data:\n",
    "                        if not dataset.loc[i, get_keys_by_value(language_dict, lang)]:\n",
    "                            continue\n",
    "                    if row.judge == \"[[TRUE]]\":\n",
    "                        true += 1\n",
    "                    else:\n",
    "                        false += 1\n",
    "    \n",
    "                acc = true / (true + false) * 100\n",
    "                if (\"Chinese\" in lang) and (\"MMO\" in data):\n",
    "                    result_dict[original_lang].append(acc)\n",
    "                else:\n",
    "                    result_dict[lang].append(acc)\n",
    "            else:\n",
    "                result_dict[lang].append(None)\n",
    "\n",
    "        result = pd.DataFrame(result_dict)\n",
    "        result.to_csv(f\"{save_path}/{data}-{model}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
